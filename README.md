# datasci266 NLP final

**Scoring AI Policies Through Applied NLP** <br><br>
**Reasure paper is here**
<br><br>
So, what is the point of this tool? well, governance is hard and sometimes it can be difficult to create governance risk and compliance documentation for new tools, esspecially with the explosive adoption of AI tools. For local government entities (municipalities, counties, state, agenceies, etc.) there are a lot more regulatory hoops to jump through to ensure responsible stewership of citizens' data and prepare for cyber threats given the higher probibility to be targeted by foreign actors. AI systems aren't new, but they are new to the commercial market and industry standards/precedents haven't been set in many sectors of business. Here in the U.S. regulatory requirments have been up and down with executive orders being issues and recinded with the new adminstration. Additionally, GRC documents can be SUPER long and time consuming to read and then determine which controls apply or don't apply to your org.<br> 
This policy evaluation tool is by no means able to replace any GRC team, but to aid GRC teams in identifying weak points and strong points for internal documentation. Just because my policy evaluation tool gives a 100% good score, that doesn't mean the document is perfect and never needs to be changed, it is just and indication that this internal document does a good job at addressing the critical categories we had outlined here in our policy evaluation methodology. It is important to point out that policy evaluation is a whole field itself with researchers who have dedicated years and even their entier lives to studying- we have grossly over simplified the many nueances that should be considered in policy evaluation. The policy evaluation approach we are taking is more inline with process evaluation and not considering impact, outcome, or cost-benefit based policy analysis.<br> 
The resulting report card from my policy evaluation tool will have 4 categories each with a possible max of 100 points (aka a percentage given for each category). The 4 critical categories we have identified are: <br>
**Govern** - covers topics like transparency, AI decision making, conducting governance, risk, compliance, security, and privacy assessments on the AI tool, is training available for workforce upskill to use AI systems responsibly, and evaluating AI use cases. <br>
**Map** - looks at language around asset management and use case inventory, are AI tools required to be explainable, are risk assessments required before deployment and are those identified risks communicated to key stakeholders? <br>
**Measure** - looks for language or requirements for measuring of fairness, accuracy, and bias in tools prior to deployment and if there is a requiremnt to continue measuring these items. Bench mark testing for AI systems and impact assessments. <br>
**Manage** - in the event of a cyber incedent or a failure of the AI system occures do any process exist to handle the situation? looking for incident response plans basically. Also looks at the operational requirements, like monitoring AI behavior and user's usage. <br>
<br>
I do think there is room to improve and I would like to do so in the future.
<br>
<br>
In this repo, you will find a lot of CSV files and some pynb files. <br>

datastuff notebook: this is where i did a majority of the data processing, i created the different datasets discussed in the paper. I have the parsing, stripping, and prepping of the text data from the synthetic data and real. There are also some other datasets I tested out from [CUAD](https://github.com/TheAtticusProject/cuad), [legalbench](https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fnguha%2Flegalbench), and [AGORA](https://www.google.com/url?q=https%3A%2F%2Fzenodo.org%2Frecords%2F15053471).
<br><br>

llama RAG notebook: this is where i have the vector db of the user's policy and the golden standard documents from [NIST](https://www.nist.gov/itl/ai-risk-management-framework) and some from AGORA. I also have the theory of mind reasoning strategy here where after the chuncks are retrieved, the model decides if these document fragments are relevent or not. for each critical area (govern, map, measure, manage) i have 2 prompts to extract relevent document fragments from both the user's submitted documents and the curated golden document set. So, for each critical area, we have the top 2 fragments for each prompt, which is meant to capture only the most important aspects of the policy documents. In theory this means the chuncks not directly evaluated are mostly title/section/apendix items, in practice, this can result in a higher chance of neutrel results as chunck retrieval is based on marginal relevence or similarity search [source](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/multi_vector/). Recognizing this showes we have room to develope upon this approach further for cases were the score for a given section is 100% neutral or is over 51% neutral. <br>
this notebook is where the first hand off takes place, after data retrievel it is readied and packages to hand off to the SSBERT notebook. <br>
Then, SSBERT will have hand back score results which are then explained here by the llama 3.1 8B instruct model. 
<br><br>


SSBERT notebook: in this notebook, we take in the csv of data from the llama RAG notebook to process and determine a score. the bert-based-uncased model is finetuned for 
the semantic similarity task with the labels ["contradiction", "entailment", "neutral"](https://nlp.stanford.edu/projects/snli/) for which i have based the scoring method off of. This is a Keras BERT pre-trained model which has some legecy dependences and thus needs the specific versions installed (in top of the nb). I used [this](https://keras.io/examples/nlp/semantic_similarity_with_bert/) implementation. I created a sentence pair dataset from the AGORA dataset which is just a collection of AI governance documents from various sources, i only used the docuemnts from united states local and federal government and some from U.S. corporations. The AGORA dataset includes text from other countries and various document types. I did label the sentence pair agora dataset using the Keras BERT model, so im not sure if that was a good call or not? anyways, the CSV from the llama RAG notebook is put into a pd dataframe which is broken down as such: 2 columns, user text and golden text; then we have the indexes going down in this order, gov1, gov2, map1, map2, mea1, mea2, man1, man2. which corispond to the 4 critical categories and the 2 prompts per category. the top 2 text fragments for each prompt are combined for a longer sample to grade. now, for each category, i count the returned labels for both text fragments, and normalize them to give the ratios. the labels count like such: entialment(+), contradiction(-), and neutral (+/-). Neutral is treated as sort of a "varience" but is not truly a measure of varience. Kind of like i mentioned in the llama notebook explaination above, if i had more time or decide to come back to this project, i want to create an additional workflow that is triggered when a high threshold of neutral is returned which will maybe retrieve the next 2 fragments from the user docs, or maybe retrieve different golden docs. I also want to test out different golden text corpuses and see if i can get different or more informitive results depending on the golden corpus. Maybe, treating the golden corpus as a module of sorts that the user can change if the current one isn't matching their use case or doesn't cover a specific secture of business. I would like like to see if any transfer learning is possible with combining tasks like legal reasoning or understandingg of contracts/legal documents to enhance the performance of the semantic similarity task. 
<br><br>

here is a diagram of the arch.
![project architecture](https://github.com/shutup-kat/datasci266NLPfinal/blob/main/projectArch%20no%20background.png)

